{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Linear Models the Easy Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linear regression fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape: (506, 13)\n"
     ]
    }
   ],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "boston = load_boston()\n",
    "print(\"\\nShape:\", boston.data.shape)\n",
    "\n",
    "X = scale(boston.data)\n",
    "\n",
    "y = boston.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing &ndash; \"scale\"\n",
    "\n",
    "The `load_boston` dataset from sklearn contains information about housing in the Boston area. Specifically, it has data on 506 houses, each with 13 features, such as the crime rate, the average number of rooms per dwelling, and the distance to employment centers.\n",
    "\n",
    "`scale(boston.data)` is a function call to the `scale` function from the `sklearn.preprocessing` module, which is used to standardize data. This function takes an input array and returns a new array with each feature centered at zero and scaled to have a standard deviation of one.\n",
    "\n",
    "So, `scale(boston.data)` standardizes the features of the Boston housing dataset. This is often done as a preprocessing step for machine learning algorithms, as it can improve the performance and stability of the models. Standardization can help ensure that features with larger magnitudes or variances don't dominate the model, and it can also help with interpretability and comparison of feature importance across models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deprecation\n",
    "\n",
    "<mark>Function load_boston is deprecated;</mark> `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
    "\n",
    "The Boston housing prices dataset has an ethical problem. You can refer to\n",
    "the documentation of this function for further details.\n",
    "\n",
    "The scikit-learn maintainers therefore strongly discourage the use of this\n",
    "dataset unless the purpose of the code is to study and educate about\n",
    "ethical issues in data science and machine learning.\n",
    "\n",
    "In this special case, you can fetch the dataset from the original\n",
    "source:\n",
    "\n",
    "```py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "```\n",
    "\n",
    "Alternative datasets include the California housing dataset (i.e.\n",
    ":func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
    "dataset. You can load the datasets as follows:\n",
    "\n",
    "```py\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "```\n",
    "\n",
    "for the California housing dataset and:\n",
    "\n",
    "```py\n",
    "from sklearn.datasets import fetch_openml\n",
    "housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
    "```\n",
    "\n",
    "for the Ames housing dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regression = LinearRegression()\n",
    "regression.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R squared: 0.7406426641094095\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean_y = np.mean(y)\n",
    "\n",
    "squared_errors_mean = np.sum((y - mean_y) ** 2)\n",
    "\n",
    "squared_errors_model = np.sum((y - regression.predict(X)) ** 2)\n",
    "\n",
    "R2 = 1 - (squared_errors_model / squared_errors_mean)\n",
    "\n",
    "print(\"\\nR squared:\", R2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [sklearn Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7406426641094095\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nScore:\", regression.score(X, y))  # Oh look!  It's the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM: -0.9', 'ZN: 1.1', 'INDUS: 0.1', 'CHAS: 0.7', 'NOX: -2.1', 'RM: 2.7', 'AGE: 0.0', 'DIS: -3.1', 'RAD: 2.7', 'TAX: -2.1', 'PTRATIO: -2.1', 'B: 0.8', 'LSTAT: -3.7']\n"
     ]
    }
   ],
   "source": [
    "print([a + \": \" + str(round(b, 1)) for a, b in \n",
    "       zip(boston.feature_names, regression.coef_, )])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What the hecc was that?\n",
    "\n",
    "This code creates a list of strings that contain the **name of each feature** in the Boston housing dataset and its **corresponding coefficient value** from a regression model.\n",
    "\n",
    "More specifically, the `zip` function takes two iterables, `boston.feature_names` and `regression.coef_`, and creates tuples from them, pairing the corresponding elements from each iterable.\n",
    "\n",
    "Here, `boston.feature_names` is a list of the names of the 13 features in the Boston housing dataset, and `regression.coef_` is a list of the regression coefficients for each feature in a previously fitted regression model.\n",
    "\n",
    "The resulting tuples are then used in a list comprehension to create a list of strings. \n",
    "\n",
    "Each string contains the name of a **feature**, followed by a **colon**, and then the **coefficient value** for that feature rounded to one decimal place.\n",
    "\n",
    "The + operator concatenates the parts of each string together.\n",
    "\n",
    "Finally, the resulting list of strings is printed to the console. This can be useful for interpreting the results of a regression model and understanding which features are most strongly associated with the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's a coefficient?\n",
    "\n",
    "In the context of a regression model, a coefficient refers to the value that represents the relationship between a predictor variable and the target variable.\n",
    "\n",
    "In linear regression, for example, a coefficient represents the slope of the line that best fits the data. It indicates the change in the value of the target variable for a one-unit change in the predictor variable, assuming all other variables are held constant.\n",
    "\n",
    "Here, `regression.coef_` refers to the coefficients estimated by a previously fitted regression model on the Boston housing dataset. Each coefficient represents the effect of a specific feature on the predicted housing price. The code prints the name of each feature along with its corresponding coefficient value, providing insight into which features are most important in predicting housing prices in the model.\n",
    "\n",
    "## Order the list of feature names and coefficient values by descending coefficient value\n",
    "\n",
    "To order the list of feature names and coefficient values by descending coefficient value, you can use the `sorted` function with the `key` argument set to the absolute value of the coefficient value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTAT: -3.7\n",
      "DIS: -3.1\n",
      "RM: 2.7\n",
      "RAD: 2.7\n",
      "TAX: -2.1\n",
      "PTRATIO: -2.1\n",
      "NOX: -2.1\n",
      "ZN: 1.1\n",
      "CRIM: -0.9\n",
      "B: 0.8\n",
      "CHAS: 0.7\n",
      "INDUS: 0.1\n",
      "AGE: 0.0\n"
     ]
    }
   ],
   "source": [
    "coef_list = [(a, b) for a, b in zip(boston.feature_names, regression.coef_)]\n",
    "sorted_coef_list = sorted(coef_list, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "for feature, coef in sorted_coef_list:\n",
    "    print(f\"{feature}: {coef:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's what's happening:\n",
    "\n",
    "1. First, a list of tuples is created containing the feature names and corresponding coefficient values, using a list comprehension: `[(a, b) for a, b in zip(boston.feature_names, regression.coef_)]`\n",
    "2. Next, the `sorted` function is called on the list of tuples. The key argument is set to a `lambda` function that takes a tuple as input and returns the absolute value of the second element (the coefficient value). This ensures that the list is sorted by descending absolute value of the coefficient. The reverse argument is set to True to sort the list in descending order: `sorted_coef_list = sorted(coef_list, key=lambda x: abs(x[1]), reverse=True)`\n",
    "3. Finally, a `for` loop is used to iterate through the sorted list and print out each feature name and coefficient value, using an f-string to format the output: `for feature, coef in sorted_coef_list: print(f\"{feature}: {coef:.1f}\")`\n",
    "\n",
    "This will print out the feature names and corresponding coefficients in descending order of absolute value of the coefficients, making it easy to see which features have the strongest association with the target variable.\n",
    "\n",
    "## What is a lambda function?\n",
    "\n",
    "A lambda function is a small, anonymous function in Python that can be defined in a single line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded:\n",
      " [[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "lbl = LabelEncoder()\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "# qualitative data\n",
    "arr = ['red', 'red', 'green', 'blue',\n",
    "       'red', 'blue', 'blue', 'green']\n",
    "\n",
    "labels = lbl.fit_transform(arr).reshape(8, 1)\n",
    "\n",
    "print(\"\\nEncoded:\\n\", enc.fit_transform(labels).toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape: (8,)\n",
      "b reshape: (8, 1)\n",
      "\n",
      "Predict b > 0.5:\n",
      " [False False False False  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "c = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "print(\"a shape:\", a.shape)\n",
    "\n",
    "b = c.reshape(8, 1)\n",
    "print(\"b reshape:\", b.shape)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regression = LinearRegression()\n",
    "regression.fit(b, a)\n",
    "\n",
    "b_pred = regression.predict(b) > 0.5  # ndarray, shape (8,)\n",
    "\n",
    "print(\"\\nPredict b > 0.5:\\n\", b_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why?\n",
    "\n",
    "In the case of `LinearRegression.fit(a, b)`, `a` is an array that contains the feature values for a set of examples, and `b` is an array that contains the target values for those examples.\n",
    "\n",
    "The shape of an array tells us how many rows and columns it has. So `(8, 1)` means there are 8 rows and 1 column, while `(8,)` means there are 8 rows and no columns (it's a one-dimensional array).\n",
    "\n",
    "When we call `LinearRegression.fit(a, b)`, the `a` array is passed in with shape `(8, 1)` and the `b` array is passed in with shape `(8,)`. This tells the `LinearRegression` object that there are 8 examples, each with 1 feature, and that the target values for those examples are stored in a one-dimensional array.\n",
    "\n",
    "The result of calling `LinearRegression.fit(a, b)` is an array of shape `(8,)`. This is because the `LinearRegression` object uses the feature values in `a` and the target values in `b` to learn a mathematical relationship between the two. Once it has learned this relationship, it can use it to make predictions for new examples.\n",
    "\n",
    "The resulting array of shape `(8,)` contains the predicted target values for each example in `a`. In other words, for each row in the `a` array, the corresponding element in the result array contains the predicted target value for that example.\n",
    "\n",
    "So, in summary, we use arrays with different shapes when working with `LinearRegression` to store the feature values and target values for a set of examples, and to store the predicted target values for those examples after the model has been trained.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In-sample accuracy: 0.979\n",
      "\n",
      "Out-of-sample accuracy: 0.958\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.cross_validation import train_test_split - cross_validation deprecated\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "binary_y = np.array(y >= 40).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, binary_y, test_size=0.33, random_state=5)\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('\\nIn-sample accuracy: %0.3f' % accuracy_score(y_train, logistic.predict(X_train)))\n",
    "\n",
    "print('\\nOut-of-sample accuracy: %0.3f' % accuracy_score(y_test, logistic.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CRIM :   0.086\n",
      "     ZN :   0.230\n",
      "  INDUS :   0.580\n",
      "   CHAS :  -0.029\n",
      "    NOX :  -0.304\n",
      "     RM :   1.769\n",
      "    AGE :  -0.127\n",
      "    DIS :  -0.539\n",
      "    RAD :   0.919\n",
      "    TAX :  -0.165\n",
      "PTRATIO :  -0.782\n",
      "      B :   0.077\n",
      "  LSTAT :  -1.628\n"
     ]
    }
   ],
   "source": [
    "for var, coef in zip(boston.feature_names, logistic.coef_[0]):\n",
    "    print(\"%7s : %7.3f\" % (var, coef))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "classes: [0 1]\n",
      "\n",
      "Probs:\n",
      " [[0.33234217 0.66765783]\n",
      " [0.97060356 0.02939644]\n",
      " [0.99594746 0.00405254]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nclasses:', logistic.classes_)\n",
    "print('\\nProbs:\\n', logistic.predict_proba(X_test)[:3, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Variable selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random features: 1 -> R2: 0.748\n",
      "Random features: 2 -> R2: 0.749\n",
      "Random features: 4 -> R2: 0.749\n",
      "Random features: 8 -> R2: 0.754\n",
      "Random features: 16 -> R2: 0.758\n",
      "Random features: 32 -> R2: 0.780\n",
      "Random features: 64 -> R2: 0.812\n",
      "Random features: 128 -> R2: 0.864\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, test_size=0.33, random_state=42)\n",
    "check = [2 ** i for i in range(8)]\n",
    "\n",
    "for i in range(2 ** 7 + 1):\n",
    "    X_train = np.column_stack((X_train, np.random.random(X_train.shape[0])))\n",
    "    X_test = np.column_stack((X_test, np.random.random(X_test.shape[0])))\n",
    "    regression.fit(X_train, y_train)\n",
    "    if i in check:\n",
    "        print(\"Random features: %i -> R2: %0.3f\" %\n",
    "              (i, r2_score(y_train, regression.predict(X_train))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 0.443\n"
     ]
    }
   ],
   "source": [
    "regression.fit(X_train, y_train)\n",
    "print('R2 %0.3f' % r2_score(y_test, regression.predict(X_test)))\n",
    "# Please notice that the R2 result may change from run to run \n",
    "# due to the random nature of the experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R2: 0.820\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "poly_X = pf.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(poly_X,\n",
    "                                                    y, test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "reg_regression = Ridge(alpha=0.1, normalize=True)\n",
    "reg_regression.fit(X_train, y_train)\n",
    "\n",
    "print('\\nR2: %0.3f' % r2_score(y_test, reg_regression.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, test_size=0.33, random_state=42)\n",
    "\n",
    "SGD = SGDRegressor(penalty=None, learning_rate='invscaling',\n",
    "                   eta0=0.01, power_t=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example      1 R2 -6.255 coef: 0.112 -0.071 0.148 -0.040 0.075 -0.021 0.146 -0.113 0.243 0.224 0.118 0.037 0.110\n",
      "\n",
      "Example      2 R2 -6.168 coef: 0.065 -0.139 0.087 -0.078 0.055 -0.114 0.254 -0.054 0.154 0.140 0.282 0.068 0.152\n",
      "\n",
      "Example      4 R2 -6.060 coef: -0.074 -0.195 0.319 -0.171 0.064 -0.206 0.527 0.048 -0.041 0.266 0.075 0.219 0.353\n",
      "\n",
      "Example      8 R2 -5.775 coef: -0.249 -0.504 0.605 -0.343 0.098 0.005 0.807 -0.304 -0.095 0.332 -0.067 0.399 0.024\n",
      "\n",
      "Example     16 R2 -5.144 coef: -0.441 -0.430 0.298 -0.571 -0.002 0.004 0.519 -0.423 -0.279 0.292 -0.544 0.665 -0.065\n",
      "\n",
      "Example     32 R2 -4.494 coef: -0.562 -0.308 0.441 1.224 0.051 0.315 0.387 -0.567 0.055 0.629 -0.367 0.726 -0.513\n",
      "\n",
      "Example     64 R2 -2.947 coef: -0.986 0.419 0.107 1.648 -0.409 1.686 -0.427 -0.201 -0.029 0.448 -1.245 1.166 -1.913\n",
      "\n",
      "Example    128 R2 -1.791 coef: -0.546 0.863 0.119 1.137 -0.584 1.823 -0.288 -0.179 -0.281 0.096 -1.982 1.165 -2.029\n",
      "\n",
      "Example    256 R2 -0.608 coef: -0.804 0.619 -0.176 1.368 -0.770 3.135 -0.304 -0.514 -0.318 -0.201 -2.325 1.238 -2.758\n",
      "\n",
      "Example    512 R2 0.289 coef: -0.665 0.455 0.167 1.302 -0.570 3.073 -0.065 -1.175 0.163 0.223 -2.238 1.074 -2.937\n",
      "\n",
      "Example   1024 R2 0.626 coef: -0.775 0.302 0.178 1.177 -0.757 3.379 -0.176 -1.477 0.308 0.216 -2.190 1.125 -3.283\n",
      "\n",
      "Example   2048 R2 0.698 coef: -0.803 0.316 0.161 1.012 -1.068 3.231 -0.303 -1.886 0.537 0.116 -2.028 1.119 -3.565\n",
      "\n",
      "Example   4096 R2 0.709 coef: -0.869 0.424 0.169 0.973 -1.362 2.988 -0.350 -2.365 0.809 -0.074 -1.931 1.101 -3.739\n",
      "\n",
      "Example   8192 R2 0.715 coef: -0.964 0.638 0.140 0.902 -1.648 2.953 -0.420 -2.688 1.120 -0.404 -1.976 1.084 -3.881\n",
      "\n",
      "Example  16384 R2 0.722 coef: -1.015 0.788 0.231 0.819 -1.800 2.713 -0.395 -2.891 1.480 -0.778 -1.991 1.069 -3.893\n",
      "\n",
      "Example  32768 R2 0.721 coef: -1.081 0.844 0.271 0.872 -1.894 2.779 -0.386 -2.977 1.779 -1.162 -2.003 1.096 -3.949\n",
      "\n",
      "Example  65536 R2 0.726 coef: -1.093 0.890 0.391 0.802 -1.882 2.697 -0.368 -2.981 1.982 -1.315 -2.020 1.077 -3.904\n",
      "\n",
      "Example 131072 R2 0.724 coef: -1.103 0.892 0.371 0.848 -1.899 2.758 -0.373 -3.003 2.027 -1.398 -2.009 1.090 -3.950\n"
     ]
    }
   ],
   "source": [
    "power = 17\n",
    "check = [2 ** i for i in range(power + 1)]\n",
    "\n",
    "for i in range(400):\n",
    "    for j in range(X_train.shape[0]):\n",
    "        SGD.partial_fit(X_train[j, :].reshape(1, 13),\n",
    "                        y_train[j].reshape(1, ))\n",
    "\n",
    "        count = (j + 1) + X_train.shape[0] * i\n",
    "\n",
    "        if count in check:\n",
    "            R2 = r2_score(y_test, SGD.predict(X_test))\n",
    "            print('\\nExample %6i R2 %0.3f coef: %s' %\n",
    "                  (count, R2, ' '.join(map(lambda x: '%0.3f' % x, SGD.coef_))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
