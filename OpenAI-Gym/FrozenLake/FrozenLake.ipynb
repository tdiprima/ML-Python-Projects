{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8b4239e",
   "metadata": {},
   "source": [
    "# Q-learning for beginners\n",
    "\n",
    "## Train an AI to solve the Frozen Lake environment\n",
    "\n",
    "Feb 13, 2022 â€¢ Maxime Labonne\n",
    "\n",
    "[Q-learning for beginners](https://mlabonne.github.io/blog/reinforcement%20learning/q-learning/frozen%20lake/gym/tutorial/2022/02/13/Q_learning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22327df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4553aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "environment = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "environment.reset()\n",
    "\n",
    "# Initialize the Q-table\n",
    "nb_states = environment.observation_space.n\n",
    "nb_actions = environment.action_space.n\n",
    "\n",
    "shape = (nb_states, nb_actions)\n",
    "qtable = np.zeros(shape)\n",
    "\n",
    "shape  # 16 tiles/states, 4 actions (L, R, U, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aab5b040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3\n",
       "0   0.0  0.0  0.0  0.0\n",
       "1   0.0  0.0  0.0  0.0\n",
       "2   0.0  0.0  0.0  0.0\n",
       "3   0.0  0.0  0.0  0.0\n",
       "4   0.0  0.0  0.0  0.0\n",
       "5   0.0  0.0  0.0  0.0\n",
       "6   0.0  0.0  0.0  0.0\n",
       "7   0.0  0.0  0.0  0.0\n",
       "8   0.0  0.0  0.0  0.0\n",
       "9   0.0  0.0  0.0  0.0\n",
       "10  0.0  0.0  0.0  0.0\n",
       "11  0.0  0.0  0.0  0.0\n",
       "12  0.0  0.0  0.0  0.0\n",
       "13  0.0  0.0  0.0  0.0\n",
       "14  0.0  0.0  0.0  0.0\n",
       "15  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(qtable)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23349147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DOWN'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly choose an action, for the hecc of it.\n",
    "seq = [\"LEFT\", \"DOWN\", \"RIGHT\", \"UP\"]\n",
    "random.choice(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcf4c964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Left 0, Down 1, Right 2, Up 3\n",
    "environment.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3fccd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THIS IS THE BASICS\n",
    "\n",
    "# Random action\n",
    "action = environment.action_space.sample()\n",
    "\n",
    "# Implement action\n",
    "new_state, reward, done, _, info = environment.step(action)\n",
    "\n",
    "# Display reward\n",
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb21b0",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "[Buckle up.](https://mlabonne.github.io/blog/reinforcement%20learning/q-learning/frozen%20lake/gym/tutorial/2022/02/13/Q_learning.html#%F0%9F%A4%96-III.-Q-learning)\n",
    "\n",
    "We need to update the value of our state-action pairs (each cell in the Q-table) considering:\n",
    "\n",
    "1. The reward for reaching the next state, and \n",
    "2. The highest possible value in the next state.\n",
    "\n",
    "The new value is the current one + the reward + the highest value in the next state.\n",
    "\n",
    "# Training\n",
    "\n",
    "So training our agent in code means:\n",
    "\n",
    "1. Choosing a random action (using `action_space.sample()`) if the values in the current state are just zeros. Otherwise, we take the action with the highest value in the current state with the function np.argmax()`\n",
    "2. Implementing this action by moving in the desired direction with `step(action)`\n",
    "3. Updating the value of the original state with the action we took, using information about the new state and the reward given by `step(action)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6440b423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table before training:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "Train time: 0.217 seconds\n",
      "\n",
      "===========================================\n",
      "Q-table after training:\n",
      "[[0.      0.59049 0.      0.     ]\n",
      " [0.      0.      0.      0.     ]\n",
      " [0.      0.      0.      0.     ]\n",
      " [0.      0.      0.      0.     ]\n",
      " [0.      0.6561  0.      0.     ]\n",
      " [0.      0.      0.      0.     ]\n",
      " [0.      0.      0.      0.     ]\n",
      " [0.      0.      0.      0.     ]\n",
      " [0.      0.      0.729   0.     ]\n",
      " [0.      0.      0.81    0.     ]\n",
      " [0.      0.9     0.      0.     ]\n",
      " [0.      0.      0.      0.     ]\n",
      " [0.      0.      0.      0.     ]\n",
      " [0.      0.      0.      0.     ]\n",
      " [0.      0.      1.      0.     ]\n",
      " [0.      0.      0.      0.     ]]\n",
      "\n",
      "outcomes len: 1000\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "\n",
    "# Re-initialize Q-table\n",
    "qtable = np.zeros((environment.observation_space.n, environment.action_space.n))\n",
    "\n",
    "# Hyperparameters\n",
    "episodes = 1000  # Total number of episodes\n",
    "alpha = 0.5  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "# List of outcomes to plot\n",
    "outcomes = []\n",
    "\n",
    "print('Q-table before training:')\n",
    "print(qtable)\n",
    "\n",
    "# Training\n",
    "for _ in range(episodes):\n",
    "    state, prob = environment.reset()\n",
    "    done = False\n",
    "\n",
    "    # By default, we consider our outcome to be a failure\n",
    "    outcomes.append(\"Failure\")\n",
    "\n",
    "    # Until the agent gets stuck in a hole or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        # Choose the action with the highest value in the current state\n",
    "        if np.max(qtable[state]) > 0:\n",
    "            action = np.argmax(qtable[state])\n",
    "        else:\n",
    "            # If there's no best action (only zeros), take a random one\n",
    "            action = environment.action_space.sample()\n",
    "\n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        new_state, reward, done, t, info = environment.step(action)\n",
    "\n",
    "        # Update Q(s,a)\n",
    "        qtable[state, action] = qtable[state, action] + \\\n",
    "                                alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n",
    "\n",
    "        # Update our current state\n",
    "        state = new_state\n",
    "\n",
    "        # If we have a reward, it means that our outcome is a success\n",
    "        if reward:\n",
    "            outcomes[-1] = \"Success\"\n",
    "\n",
    "end_time = timer()\n",
    "total_time = end_time - start_time\n",
    "print(f\"\\nTrain time: {total_time:.3f} seconds\")\n",
    "\n",
    "print()\n",
    "print('===========================================')\n",
    "print('Q-table after training:')\n",
    "print(qtable)\n",
    "print(\"\\noutcomes len:\", len(outcomes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b39f8",
   "metadata": {},
   "source": [
    "# Evaluating trained agent on 100 episodes\n",
    "\n",
    "Calculate the percentage of times the agent managed to reach the goal (success rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "651f41a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num successes: 100.0\n",
      "\n",
      "episodes: 100\n",
      "\n",
      "Success rate = 100.0%\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "nb_success = 0\n",
    "\n",
    "# Evaluation\n",
    "for _ in range(100):\n",
    "    state, p = environment.reset()\n",
    "    done = False\n",
    "\n",
    "    # Do until agent gets stuck or reaches goal\n",
    "    while not done:\n",
    "        # Choose action with highest value\n",
    "        if np.max(qtable[state]) > 0:\n",
    "            action = np.argmax(qtable[state])\n",
    "        else:\n",
    "            # If no best action, take random one\n",
    "            action = environment.action_space.sample()\n",
    "\n",
    "        # Implement action and move agent\n",
    "        new_state, reward, done, tt, info = environment.step(action)\n",
    "\n",
    "        # Update current state\n",
    "        state = new_state\n",
    "\n",
    "        # Tally up the reward\n",
    "        nb_success += reward\n",
    "\n",
    "print(\"\\nnum successes:\", nb_success)\n",
    "\n",
    "print(\"\\nepisodes:\", episodes)\n",
    "\n",
    "print(f\"\\nSuccess rate = {nb_success / episodes * 100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef5d81",
   "metadata": {},
   "source": [
    "# Visualize the agent moving on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5238e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# import gym\n",
    "# import numpy as np\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "# environment = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"human\")\n",
    "\n",
    "# nb_states = environment.observation_space.n\n",
    "# nb_actions = environment.action_space.n\n",
    "# qtable = np.zeros((nb_states, nb_actions))\n",
    "\n",
    "# state, p = environment.reset()\n",
    "# environment.render()\n",
    "\n",
    "# done = False\n",
    "# sequence = []\n",
    "\n",
    "# while not done:\n",
    "#     if np.max(qtable[state]) > 0:\n",
    "#         action = np.argmax(qtable[state])\n",
    "#     else:\n",
    "#         action = environment.action_space.sample()\n",
    "\n",
    "#     # Add the action to the sequence\n",
    "#     sequence.append(action)\n",
    "\n",
    "#     new_state, reward, done, t, info = environment.step(action)\n",
    "\n",
    "#     state = new_state\n",
    "\n",
    "#     # Update the render\n",
    "#     clear_output(wait=True)\n",
    "#     environment.render()\n",
    "#     time.sleep(1)\n",
    "\n",
    "# print(f\"Sequence = {sequence}\")\n",
    "\n",
    "# environment.close()\n",
    "\n",
    "# Sequence = [1, 3, 0, 2, 0, 1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910cb7d0",
   "metadata": {},
   "source": [
    "# Epsilon-Greedy algorithm\n",
    "\n",
    "With our previous approach, the agent always chooses the action with the highest value.\n",
    "\n",
    "We want to allow our agent to either:\n",
    "\n",
    "1. Take the action with the highest value (exploitation)\n",
    "2. Choose a random action to try to find even better ones (exploration)\n",
    "\n",
    "## Implement a linear decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42c15e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table before training:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "===========================================\n",
      "Q-table after training:\n",
      "[[0.531441   0.59049    0.59049    0.531441  ]\n",
      " [0.531441   0.         0.6561     0.59033723]\n",
      " [0.58560004 0.729      0.55250441 0.65566461]\n",
      " [0.63222237 0.         0.33930303 0.11399433]\n",
      " [0.59049    0.6561     0.         0.531441  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.81       0.         0.64356913]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6561     0.         0.729      0.59049   ]\n",
      " [0.6561     0.81       0.81       0.        ]\n",
      " [0.729      0.9        0.         0.72899988]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.80999991 0.9        0.72899999]\n",
      " [0.81       0.9        1.         0.81      ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Train time: 0.233 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "\n",
    "# Reset qtable\n",
    "qtable = np.zeros((environment.observation_space.n, environment.action_space.n))\n",
    "\n",
    "# Hyper-parameters\n",
    "episodes = 1000  # Total number of episodes\n",
    "alpha = 0.5  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "# ADD EPSILON\n",
    "epsilon = 1.0  # Amount of randomness in the action selection\n",
    "epsilon_decay = 0.001  # Fixed amount to decrease\n",
    "\n",
    "outcomes = []\n",
    "\n",
    "print('Q-table before training:')\n",
    "print(qtable)\n",
    "\n",
    "# Training\n",
    "for _ in range(episodes):\n",
    "    state, p = environment.reset()\n",
    "    done = False\n",
    "\n",
    "    outcomes.append(\"Failure\")\n",
    "\n",
    "    while not done:\n",
    "        # Generate random number between 0 and 1\n",
    "        rnd = np.random.random()\n",
    "\n",
    "        # INSTEAD OF: if np.max(qtable[state]) > 0: action = np.argmax(qtable[state]);\n",
    "        # DO: If random number < epsilon, take a random action\n",
    "        if rnd < epsilon:\n",
    "            action = environment.action_space.sample()\n",
    "        else:\n",
    "            # You get an array of 4 values; you pick the max value.\n",
    "            action = np.argmax(qtable[state])\n",
    "\n",
    "        new_state, reward, done, t, info = environment.step(action)\n",
    "\n",
    "        # Update Q(s,a)\n",
    "        qtable[state, action] = qtable[state, action] + \\\n",
    "                                alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        if reward:\n",
    "            outcomes[-1] = \"Success\"\n",
    "\n",
    "    # Update epsilon\n",
    "    epsilon = max(epsilon - epsilon_decay, 0)\n",
    "\n",
    "end_time = timer()\n",
    "\n",
    "print()\n",
    "print('===========================================')\n",
    "print('Q-table after training:')\n",
    "print(qtable)\n",
    "\n",
    "total_time = end_time - start_time\n",
    "print(f\"\\nTrain time: {total_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d6753",
   "metadata": {},
   "source": [
    "### Not bad!\n",
    "\n",
    "More of the table is filled in.  But how did it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96282fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate = 100.0%\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "nb_success = 0\n",
    "\n",
    "# Evaluation\n",
    "for _ in range(100):\n",
    "    state, p = environment.reset()\n",
    "    done = False\n",
    "\n",
    "    # Until the agent gets stuck or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        # Choose the action with the highest value in the current state\n",
    "        action = np.argmax(qtable[state])\n",
    "\n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        new_state, reward, done, t, info = environment.step(action)\n",
    "\n",
    "        # Update our current state\n",
    "        state = new_state\n",
    "\n",
    "        # When we get a reward, it means we solved the game\n",
    "        nb_success += reward\n",
    "\n",
    "# Let's check our success rate!\n",
    "print(f\"Success rate = {nb_success / episodes * 100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b7be58",
   "metadata": {},
   "source": [
    "## Nice!\n",
    "\n",
    "# Slippery frozen lake?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2b35f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table before training:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "===========================================\n",
      "Q-table after training:\n",
      "[[0.0747668  0.02380324 0.02559581 0.02514055]\n",
      " [0.01810044 0.01297042 0.01014918 0.02246237]\n",
      " [0.01734818 0.02262023 0.02817611 0.02821268]\n",
      " [0.00440419 0.01710293 0.01713482 0.02780391]\n",
      " [0.08572783 0.02313136 0.02033653 0.02205256]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.00141386 0.00151028 0.0019543  0.00171982]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05542625 0.04201732 0.01927871 0.18337886]\n",
      " [0.08799223 0.35035972 0.04802661 0.09908065]\n",
      " [0.5502953  0.07968777 0.05927805 0.0775592 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.12861417 0.182925   0.61739371 0.0638344 ]\n",
      " [0.40841728 0.95570518 0.39422477 0.39612477]\n",
      " [0.         0.         0.         0.        ]]\n",
      "Success rate = 90.0%\n"
     ]
    }
   ],
   "source": [
    "environment = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "environment.reset()\n",
    "\n",
    "# RESET Q-table\n",
    "qtable = np.zeros((environment.observation_space.n, environment.action_space.n))\n",
    "\n",
    "# Hyper-parameters\n",
    "episodes = 1000  # Total number of episodes\n",
    "alpha = 0.5  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 1.0  # Amount of randomness in the action selection\n",
    "epsilon_decay = 0.001  # Fixed amount to decrease\n",
    "\n",
    "# List of outcomes to plot\n",
    "outcomes = []\n",
    "\n",
    "print('Q-table before training:')\n",
    "print(qtable)\n",
    "\n",
    "# TRAIN\n",
    "for _ in range(episodes):\n",
    "    state, prob = environment.reset()\n",
    "    done = False\n",
    "\n",
    "    # By default, we consider our outcome to be a failure\n",
    "    outcomes.append(\"Failure\")\n",
    "\n",
    "    # Until the agent gets stuck in a hole or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        # Generate a random number between 0 and 1\n",
    "        rnd = np.random.random()\n",
    "\n",
    "        # If random number < epsilon, take a random action\n",
    "        if rnd < epsilon:\n",
    "            action = environment.action_space.sample()\n",
    "        else:\n",
    "            # Else, take the action with the highest value in the current state\n",
    "            action = np.argmax(qtable[state])\n",
    "\n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        new_state, reward, done, t, info = environment.step(action)\n",
    "\n",
    "        # Update Q(s,a)\n",
    "        qtable[state, action] = qtable[state, action] + \\\n",
    "                                alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n",
    "\n",
    "        # Update our current state\n",
    "        state = new_state\n",
    "\n",
    "        # If we have a reward, it means that our outcome is a success\n",
    "        if reward:\n",
    "            outcomes[-1] = \"Success\"\n",
    "\n",
    "    # Update epsilon\n",
    "    epsilon = max(epsilon - epsilon_decay, 0)\n",
    "\n",
    "print()\n",
    "print('===========================================')\n",
    "print('Q-table after training:')\n",
    "print(qtable)\n",
    "\n",
    "# EVALUATE\n",
    "episodes = 100\n",
    "nb_success = 0\n",
    "\n",
    "for _ in range(100):\n",
    "    state, p = environment.reset()\n",
    "    done = False\n",
    "\n",
    "    # Until the agent gets stuck or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        # Choose the action with the highest value in the current state\n",
    "        action = np.argmax(qtable[state])\n",
    "\n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        new_state, reward, done, t, info = environment.step(action)\n",
    "\n",
    "        # Update our current state\n",
    "        state = new_state\n",
    "\n",
    "        # When we get a reward, it means we solved the game\n",
    "        nb_success += reward\n",
    "\n",
    "# Let's check our success rate!\n",
    "print(f\"Success rate = {nb_success / episodes * 100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590cf1e",
   "metadata": {},
   "source": [
    "# Nice!\n",
    "\n",
    "Success rate is 90%.\n",
    "\n",
    "What if it wasn't; what if we wanna make it better?\n",
    "\n",
    "You can tweak the hyper-parameters...\n",
    "\n",
    "Maybe implement exponential decay for the epsilon-greedy algorithm too...\n",
    "\n",
    "FYI &ndash; slightly modifying the hyperparameters can completely destroy the results.\n",
    "\n",
    "This is a quirk of reinforcement learning: hyperparameters are quite moody, and it is important to understand their meaning if you want to tweak them.\n",
    "\n",
    "It's always good to test and try new combinations to build your intuition and become more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9507151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
